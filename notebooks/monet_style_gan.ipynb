{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0d585b",
   "metadata": {},
   "source": [
    "# Monet Style GAN — Baseline Notebook\n",
    "This notebook scaffolds a Kaggle submission for the *I’m Something of a Painter Myself* competition. It trains a GAN to synthesise Monet-style paintings at 256×256 resolution and packages 8,000 generated JPEGs into `images.zip`.\n",
    "\n",
    "**Competition goal:** Translate real-world landscape photos into Claude Monet-inspired artwork by learning a GAN on the official Kaggle dataset. Final submissions are evaluated by the Monet Fréchet Inception Distance (MiFID) metric on hidden test images.\n",
    "\n",
    "**Dataset source:** Local copy of Kaggle’s `gan-getting-started` bundle extracted under `data/kaggle_raw/monet_jpg` (synchronised from <https://www.kaggle.com/competitions/gan-getting-started/data>). All experiments below read Monet canvases from that directory.\n",
    "\n",
    "**Citation:** Kaggle. *I’m Something of a Painter Myself* competition. Available at <https://www.kaggle.com/competitions/gan-getting-started>. Accessed 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a89fe",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "1. Locate or extract the Monet paintings dataset (from Kaggle’s `gan-getting-started` bundle or a local copy).\n",
    "2. Build a data pipeline that yields 256×256 RGB tensors with light augmentation.\n",
    "3. Train a deep DCGAN-style generator/discriminator pair tuned for high-resolution art.\n",
    "4. Optionally inspect training curves or qualitative samples during experimentation.\n",
    "5. Generate 8,000 Monet-style images directly into `images.zip` for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac4d5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet torch torchvision --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4f1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tamohammad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import platform\n",
    "import random\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Reproducibility helpers\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(2025)\n",
    "\n",
    "IS_KAGGLE = Path('/kaggle').exists()\n",
    "WORKING_DIR = Path('/kaggle/working') if IS_KAGGLE else Path.cwd()\n",
    "PROJECT_ROOT = WORKING_DIR\n",
    "if not IS_KAGGLE and PROJECT_ROOT.name == 'notebooks':\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "DATA_ROOT_CANDIDATES: List[Path] = []\n",
    "if IS_KAGGLE:\n",
    "    DATA_ROOT_CANDIDATES.extend([Path('/kaggle/input/gan-getting-started'), Path('/kaggle/input')])\n",
    "local_data_dirs = [PROJECT_ROOT / 'data', PROJECT_ROOT / 'input', PROJECT_ROOT / 'datasets']\n",
    "for local_candidate in local_data_dirs:\n",
    "    if local_candidate.exists():\n",
    "        DATA_ROOT_CANDIDATES.append(local_candidate)\n",
    "DATA_ROOT_CANDIDATES.append(WORKING_DIR)\n",
    "\n",
    "SYSTEM = platform.system()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed512e",
   "metadata": {},
   "source": [
    "## Data Pipeline\n",
    "The helper below searches common Kaggle and local paths for a folder named `monet_jpg`. If only a zip archive is found, it is extracted once into the working directory. Update `CANDIDATE_DATASETS` if you use an alternative source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc09c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('d:/MS_in_AI/WK_5_Dog_Gen/WK5_Generative_Dog_Images/data/kaggle_raw/monet_jpg')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CANDIDATE_DATASETS = ['monet_jpg', 'monet', 'monet-paintings']\n",
    "ARCHIVE_PATTERNS = ['monet_jpg*.zip', 'monet-paintings*.zip']\n",
    "\n",
    "def locate_monet_images() -> Path:\n",
    "    for root in DATA_ROOT_CANDIDATES:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "        for name in CANDIDATE_DATASETS:\n",
    "            candidate = root / name\n",
    "            if candidate.is_dir() and any(candidate.glob('*.jpg')):\n",
    "                return candidate\n",
    "        for match in root.glob('**/monet_jpg'):\n",
    "            if match.is_dir() and any(match.glob('*.jpg')):\n",
    "                return match\n",
    "    archives = []\n",
    "    for root in DATA_ROOT_CANDIDATES:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "        for pattern in ARCHIVE_PATTERNS:\n",
    "            archives.extend(root.glob(f'**/{pattern}'))\n",
    "    if archives:\n",
    "        extract_root = WORKING_DIR / 'monet-extracted'\n",
    "        if not extract_root.exists():\n",
    "            extract_root.mkdir(parents=True, exist_ok=True)\n",
    "            for archive in archives:\n",
    "                with zipfile.ZipFile(archive) as zf:\n",
    "                    zf.extractall(extract_root)\n",
    "        for match in extract_root.glob('**/monet_jpg'):\n",
    "            if match.is_dir() and any(match.glob('*.jpg')):\n",
    "                return match\n",
    "    raise FileNotFoundError('Unable to locate Monet paintings. Place a monet_jpg/ folder or zip under data/.')\n",
    "\n",
    "IMAGE_ROOT = locate_monet_images()\n",
    "IMAGE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185e184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "IMAGE_SIZE = 256\n",
    "NUM_WORKERS = 0 if SYSTEM == 'Windows' else 2\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0), ratio=(0.9, 1.1)) if IS_KAGGLE else transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.15, hue=0.03),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fe74e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MonetDataset(Dataset):\n",
    "    def __init__(self, root: Path, transform: transforms.Compose):\n",
    "        self.files = sorted(str(p) for p in root.glob('*.jpg'))\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f'No JPEGs found in {root}.')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        path = self.files[idx]\n",
    "        with Image.open(path) as img:\n",
    "            img = img.convert('RGB')\n",
    "        return self.transform(img), 0  # dummy label for compatibility\n",
    "\n",
    "dataset = MonetDataset(IMAGE_ROOT, train_transform)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=(device.type == 'cuda'), drop_last=True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ad16d",
   "metadata": {},
   "source": [
    "## Exploratory Data Snapshot\n",
    "Quick visual check of Monet canvases ensures transforms and colour statistics look sensible before training the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch, _ = next(iter(data_loader))\n",
    "grid = utils.make_grid(sample_batch[:16], nrow=4, normalize=True, value_range=(-1, 1))\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.title('Random Monet paintings (after augmentation)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab2ba3",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- Palette skews toward soft blues, greens, and ochres; augmentations keep tones within Monet’s range.\n",
    "- Random crops preserve large structures (bridges, water reflections) which should help the discriminator distinguish composition.\n",
    "- Moderate colour jitter adds diversity without breaking artistic coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3da50c9",
   "metadata": {},
   "source": [
    "## DCGAN Architecture\n",
    "The generator upsamples a latent vector to 256×256 while the discriminator mirrors the process. BatchNorm and learning-rate-friendly Adam optimisers keep training stable without extending runtime too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8baae5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 128\n",
    "GEN_FEATURES = 64\n",
    "DISC_FEATURES = 64\n",
    "\n",
    "def gen_block(in_ch: int, out_ch: int) -> Iterable[nn.Module]:\n",
    "    return (\n",
    "        nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(out_ch),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "def disc_block(in_ch: int, out_ch: int, *, use_bn: bool = True) -> Iterable[nn.Module]:\n",
    "    layers = [nn.Conv2d(in_ch, out_ch, 4, 2, 1, bias=False)]\n",
    "    if use_bn:\n",
    "        layers.append(nn.BatchNorm2d(out_ch))\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return layers\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim: int, feature_maps: int, channels: int = 3):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = [\n",
    "            nn.ConvTranspose2d(latent_dim, feature_maps * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps * 16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        layers.extend(gen_block(feature_maps * 16, feature_maps * 8))\n",
    "        layers.extend(gen_block(feature_maps * 8, feature_maps * 4))\n",
    "        layers.extend(gen_block(feature_maps * 4, feature_maps * 2))\n",
    "        layers.extend(gen_block(feature_maps * 2, feature_maps))\n",
    "        layers.extend(gen_block(feature_maps, feature_maps // 2))\n",
    "        layers.append(nn.ConvTranspose2d(feature_maps // 2, channels, 4, 2, 1, bias=False))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_maps: int, channels: int = 3):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = [\n",
    "            nn.Conv2d(channels, feature_maps // 2, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        layers.extend(disc_block(feature_maps // 2, feature_maps))\n",
    "        layers.extend(disc_block(feature_maps, feature_maps * 2))\n",
    "        layers.extend(disc_block(feature_maps * 2, feature_maps * 4))\n",
    "        layers.extend(disc_block(feature_maps * 4, feature_maps * 8))\n",
    "        layers.extend(disc_block(feature_maps * 8, feature_maps * 16))\n",
    "        layers.append(nn.Conv2d(feature_maps * 16, 1, 4, 1, 0, bias=False))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d6b65dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module: nn.Module) -> None:\n",
    "    classname = module.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(module.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(module.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(module.bias.data, 0)\n",
    "\n",
    "\n",
    "generator = Generator(LATENT_DIM, GEN_FEATURES).to(device)\n",
    "discriminator = Discriminator(DISC_FEATURES).to(device)\n",
    "generator.apply(init_weights)\n",
    "discriminator.apply(init_weights)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optimizer_g, gamma=0.99)\n",
    "scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optimizer_d, gamma=0.99)\n",
    "\n",
    "fixed_noise = torch.randn(64, LATENT_DIM, 1, 1, device=device)\n",
    "\n",
    "\n",
    "def save_state_dict(target_path: Path, state: dict) -> None:\n",
    "    temp_path = (\n",
    "        target_path.with_suffix(target_path.suffix + '.tmp')\n",
    "        if target_path.suffix\n",
    "        else target_path.with_name(target_path.name + '.tmp')\n",
    "    )\n",
    "    if temp_path.exists():\n",
    "        temp_path.unlink()\n",
    "    try:\n",
    "        torch.save(state, temp_path, _use_new_zipfile_serialization=False)\n",
    "        temp_path.replace(target_path)\n",
    "    except Exception:\n",
    "        if temp_path.exists():\n",
    "            temp_path.unlink(missing_ok=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b36e4",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "This loop tracks generator/discriminator losses, saves checkpoints, and emits progress images to `/kaggle/working/samples`.\n",
    "\n",
    "**Key hyperparameters**\n",
    "- Latent dim: 128 (normal noise)\n",
    "- Batch size: 8 (Windows CPU-friendly)\n",
    "- Learning rate: 2e-4 for both optimisers with betas (0.5, 0.999)\n",
    "- Epochs: configurable per run (default 3 locally, increase on Kaggle GPU)\n",
    "- Augmentations: random crop/flip, mild colour jitter\n",
    "\n",
    "Adjust `NUM_EPOCHS`, `LOG_INTERVAL`, and learning rates to balance runtime versus MiFID quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6684cb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best generator checkpoint (epoch 1).\n",
      "Saved new best generator checkpoint (epoch 3).\n",
      "Saved new best generator checkpoint (epoch 3).\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3  # tweak as needed to fit kernel runtime budget\n",
    "LOG_INTERVAL = 50\n",
    "CHECKPOINT_DIR = WORKING_DIR / \"checkpoints\"\n",
    "SAMPLES_DIR = WORKING_DIR / \"samples\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_g_loss = math.inf\n",
    "history = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    for step, (real_imgs, _) in enumerate(data_loader, start=1):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        batch_size = real_imgs.size(0)\n",
    "\n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(batch_size, LATENT_DIM, 1, 1, device=device)\n",
    "        fake_imgs = generator(noise).detach()\n",
    "        optimizer_d.zero_grad(set_to_none=True)\n",
    "        real_targets = torch.ones(batch_size, device=device)\n",
    "        fake_targets = torch.zeros(batch_size, device=device)\n",
    "        real_logits = discriminator(real_imgs)\n",
    "        fake_logits = discriminator(fake_imgs)\n",
    "        loss_d = criterion(real_logits, real_targets) + criterion(fake_logits, fake_targets)\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train Generator\n",
    "        noise = torch.randn(batch_size, LATENT_DIM, 1, 1, device=device)\n",
    "        optimizer_g.zero_grad(set_to_none=True)\n",
    "        generated = generator(noise)\n",
    "        gen_logits = discriminator(generated)\n",
    "        loss_g = criterion(gen_logits, real_targets)\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            print(f\"Epoch {epoch:03d}/{NUM_EPOCHS} | Step {step:04d}/{len(data_loader)} | D: {loss_d.item():.3f} | G: {loss_g.item():.3f}\")\n",
    "            with torch.no_grad():\n",
    "                preview = generator(fixed_noise).cpu()\n",
    "                utils.save_image(preview, SAMPLES_DIR / f\"epoch_{epoch:03d}_step_{step:04d}.png\",\n",
    "                                 nrow=8, normalize=True, value_range=(-1, 1))\n",
    "\n",
    "    scheduler_g.step()\n",
    "    scheduler_d.step()\n",
    "    mean_g = loss_g.item()\n",
    "    history.append({\"epoch\": epoch, \"generator_loss\": mean_g, \"discriminator_loss\": loss_d.item()})\n",
    "    if mean_g < best_g_loss:\n",
    "        best_g_loss = mean_g\n",
    "        save_state_dict(CHECKPOINT_DIR / \"generator_best.pt\", generator.state_dict())\n",
    "        save_state_dict(CHECKPOINT_DIR / \"discriminator_best.pt\", discriminator.state_dict())\n",
    "        print(f\"Saved new best generator checkpoint (epoch {epoch}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf90d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if history:\n",
    "    history_df = pd.DataFrame(history)\n",
    "    display(history_df.head())\n",
    "    ax = history_df.plot(x='epoch', y=['generator_loss', 'discriminator_loss'], marker='o', figsize=(6, 4))\n",
    "    ax.set_title('Training Losses by Epoch')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('History is empty. Run the training cell first to populate metrics.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dfff1c",
   "metadata": {},
   "source": [
    "**Results & limitations:**\n",
    "- Generator loss trends downward faster than discriminator, signalling reasonable training balance during early epochs.\n",
    "- CPU-only run is slow; expect far smoother curves on Kaggle’s GPU where more epochs are feasible.\n",
    "- No quantitative MiFID is computed offline; evaluation still depends on Kaggle submission feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9740a81",
   "metadata": {},
   "source": [
    "### Optional: Inspect Training History\n",
    "The dictionary `history` contains per-epoch losses. Convert it to a DataFrame or plot within Kaggle for deeper analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb1f2c",
   "metadata": {},
   "source": [
    "## (Optional) Metric Prototyping\n",
    "MiFID cannot be reproduced exactly offline, but public FID approximations are informative. You can enable the cell below once `torchmetrics` is available (already shipped in Kaggle notebooks). Skip during competition runs to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f11c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "# fid_metric = FrechetInceptionDistance(feature=2048).to(device)\n",
    "# generator.eval()\n",
    "# with torch.no_grad():\n",
    "#     for real_imgs, _ in data_loader:\n",
    "#         real_imgs = real_imgs.to(device)\n",
    "#         fid_metric.update(real_imgs, real=True)\n",
    "#         noise = torch.randn(real_imgs.size(0), LATENT_DIM, 1, 1, device=device)\n",
    "#         fake_imgs = generator(noise)\n",
    "#         fid_metric.update(fake_imgs, real=False)\n",
    "# fid_score = fid_metric.compute().item()\n",
    "# print(f\"Approximate FID: {fid_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde785ae",
   "metadata": {},
   "source": [
    "## Generate Submission Images\n",
    "Load the best generator checkpoint, sample 10,000 latent vectors in manageable batches, save PNGs, and zip them to `images.zip`. Kaggle will pick up the archive from `/kaggle/working`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ad41773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Monet-style images: 100%|██████████| 8000/8000 [02:46<00:00, 48.04it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created submission archive at d:\\MS_in_AI\\WK_5_Dog_Gen\\WK5_Generative_Dog_Images\\notebooks\\images.zip\n"
     ]
    }
   ],
   "source": [
    "SUBMISSION_IMAGES = 8_000\n",
    "GEN_BATCH = 64\n",
    "OUTPUT_ZIP = WORKING_DIR / 'images.zip'\n",
    "TEMP_IMAGE_DIR = WORKING_DIR / 'submission_images'\n",
    "\n",
    "checkpoint_path = CHECKPOINT_DIR / 'generator_best.pt'\n",
    "if checkpoint_path.exists():\n",
    "    generator.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "generator.eval()\n",
    "TEMP_IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    produced = 0\n",
    "    progress = tqdm(total=SUBMISSION_IMAGES, desc='Generating Monet-style images')\n",
    "    while produced < SUBMISSION_IMAGES:\n",
    "        current = min(GEN_BATCH, SUBMISSION_IMAGES - produced)\n",
    "        noise = torch.randn(current, LATENT_DIM, 1, 1, device=device)\n",
    "        fake_batch = generator(noise).cpu()\n",
    "        for idx in range(current):\n",
    "            filename = TEMP_IMAGE_DIR / f'monet_{produced + idx:05d}.jpg'\n",
    "            utils.save_image(fake_batch[idx], filename, normalize=True, value_range=(-1, 1))\n",
    "        produced += current\n",
    "        progress.update(current)\n",
    "    progress.close()\n",
    "\n",
    "with zipfile.ZipFile(OUTPUT_ZIP, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for img_path in sorted(TEMP_IMAGE_DIR.glob('*.jpg')):\n",
    "        zf.write(img_path, arcname=img_path.name)\n",
    "print(f'Created submission archive at {OUTPUT_ZIP}')\n",
    "\n",
    "# Clean up intermediate files\n",
    "for img_path in TEMP_IMAGE_DIR.glob('*.jpg'):\n",
    "    img_path.unlink()\n",
    "TEMP_IMAGE_DIR.rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba599d0",
   "metadata": {},
   "source": [
    "## Summary & Reproducibility Notes\n",
    "- **What worked:** end-to-end DCGAN pipeline, checkpointing via `save_state_dict`, and submission image generator validated on local data.\n",
    "- **What needs tuning:** extend training on GPU hardware, explore architectural upgrades (EMA, attention, StyleGAN2), and iterate based on Kaggle MiFID feedback.\n",
    "- **Reproducibility pointers:** keep data under `data/kaggle_raw/`, run cells sequentially after `seed_everything`, and sync the notebook plus helper scripts to the public GitHub repo referenced in the introduction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
