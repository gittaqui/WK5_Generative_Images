{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb63bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f49a1",
   "metadata": {},
   "source": [
    "# Monet Style GAN — Kaggle Kernel\n",
    "This notebook mirrors the local baseline but is streamlined for execution inside Kaggle's *I'm Something of a Painter Myself* competition environment. It trains a DCGAN on the provided Monet paintings and exports the required `images.zip` submission bundle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2411ff",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "In Kaggle's runtime the correct CPU/GPU builds of PyTorch are already available. Uncomment the next cell only if you encounter a missing dependency in a custom environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bff9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baca30fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tamohammad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(2025)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900fc1de",
   "metadata": {},
   "source": [
    "## Data Pipeline\n",
    "Kaggle exposes the Monet dataset under `/kaggle/input/gan-getting-started/monet_jpg`. The helper below confirms availability and prepares a lightweight augmentation stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3023e0d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Expected Monet data under /kaggle/input/gan-getting-started/monet_jpg",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m DATA_ROOT = Path(\u001b[33m'\u001b[39m\u001b[33m/kaggle/input/gan-getting-started/monet_jpg\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DATA_ROOT.exists():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mExpected Monet data under /kaggle/input/gan-getting-started/monet_jpg\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m BATCH_SIZE = \u001b[32m16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device.type == \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m8\u001b[39m\n\u001b[32m      6\u001b[39m IMAGE_SIZE = \u001b[32m256\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Expected Monet data under /kaggle/input/gan-getting-started/monet_jpg"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path('/kaggle/input/gan-getting-started/monet_jpg')\n",
    "if not DATA_ROOT.exists():\n",
    "    raise FileNotFoundError('Expected Monet data under /kaggle/input/gan-getting-started/monet_jpg')\n",
    "\n",
    "BATCH_SIZE = 16 if device.type == 'cuda' else 8\n",
    "IMAGE_SIZE = 256\n",
    "NUM_WORKERS = 2 if device.type == 'cuda' else 0\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.15, hue=0.03),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9eb25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonetDataset(Dataset):\n",
    "    def __init__(self, root: Path, transform: transforms.Compose):\n",
    "        self.files = sorted(root.glob('*.jpg'))\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f'No JPEGs found in {root}.')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        path = self.files[idx]\n",
    "        with Image.open(path) as img:\n",
    "            img = img.convert('RGB')\n",
    "        return self.transform(img), 0\n",
    "\n",
    "dataset = MonetDataset(DATA_ROOT, train_transform)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=(device.type == 'cuda'), drop_last=True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6950a5f",
   "metadata": {},
   "source": [
    "### Exploratory Batch\n",
    "Quick visual sanity check ensures augmentations look reasonable before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch, _ = next(iter(data_loader))\n",
    "grid = utils.make_grid(sample_batch[:16], nrow=4, normalize=True, value_range=(-1, 1))\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.title('Random Monet paintings (augmented)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7965da3",
   "metadata": {},
   "source": [
    "## DCGAN Model\n",
    "Generator and discriminator mirror the architecture recommended in the competition discussion threads with minor depth tweaks for 256×256 resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ade0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 128\n",
    "GEN_FEATURES = 64\n",
    "DISC_FEATURES = 64\n",
    "\n",
    "def gen_block(in_ch: int, out_ch: int) -> Iterable[nn.Module]:\n",
    "    return (\n",
    "        nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(out_ch),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "def disc_block(in_ch: int, out_ch: int, *, use_bn: bool = True) -> Iterable[nn.Module]:\n",
    "    layers = [nn.Conv2d(in_ch, out_ch, 4, 2, 1, bias=False)]\n",
    "    if use_bn:\n",
    "        layers.append(nn.BatchNorm2d(out_ch))\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return layers\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim: int, feature_maps: int, channels: int = 3):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = [\n",
    "            nn.ConvTranspose2d(latent_dim, feature_maps * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps * 16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        layers.extend(gen_block(feature_maps * 16, feature_maps * 8))\n",
    "        layers.extend(gen_block(feature_maps * 8, feature_maps * 4))\n",
    "        layers.extend(gen_block(feature_maps * 4, feature_maps * 2))\n",
    "        layers.extend(gen_block(feature_maps * 2, feature_maps))\n",
    "        layers.extend(gen_block(feature_maps, feature_maps // 2))\n",
    "        layers.append(nn.ConvTranspose2d(feature_maps // 2, channels, 4, 2, 1, bias=False))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_maps: int, channels: int = 3):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = [\n",
    "            nn.Conv2d(channels, feature_maps // 2, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        layers.extend(disc_block(feature_maps // 2, feature_maps))\n",
    "        layers.extend(disc_block(feature_maps, feature_maps * 2))\n",
    "        layers.extend(disc_block(feature_maps * 2, feature_maps * 4))\n",
    "        layers.extend(disc_block(feature_maps * 4, feature_maps * 8))\n",
    "        layers.extend(disc_block(feature_maps * 8, feature_maps * 16))\n",
    "        layers.append(nn.Conv2d(feature_maps * 16, 1, 4, 1, 0, bias=False))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x).view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f4679",
   "metadata": {},
   "source": [
    "### Architectural Rationale\n",
    "- **Generator:** Progressive transposed convolutions double spatial resolution each block, keeping feature maps high until late layers for richer textures.\n",
    "- **Discriminator:** Mirror depth with LeakyReLU activations to stabilise gradients while BN preserves signal.\n",
    "- **Latent size:** `LATENT_DIM = 128` balances expressivity and convergence time; experiment with 256 for more detail if GPU budget allows.\n",
    "- **Feature scaling:** Adjust `GEN_FEATURES` / `DISC_FEATURES` to grow or shrink the network based on leaderboard feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78220db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module: nn.Module) -> None:\n",
    "    classname = module.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(module.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(module.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(module.bias.data, 0)\n",
    "\n",
    "generator = Generator(LATENT_DIM, GEN_FEATURES).to(device)\n",
    "discriminator = Discriminator(DISC_FEATURES).to(device)\n",
    "generator.apply(init_weights)\n",
    "discriminator.apply(init_weights)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "G_LR = 2e-4\n",
    "D_LR = 2e-4\n",
    "BETA1, BETA2 = 0.5, 0.999\n",
    "REAL_LABEL = 0.9\n",
    "FAKE_LABEL = 0.1\n",
    "GEN_TARGET = 1.0\n",
    "MAX_PREVIEW_SNAPSHOTS = 12\n",
    "\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=G_LR, betas=(BETA1, BETA2))\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=D_LR, betas=(BETA1, BETA2))\n",
    "scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optimizer_g, gamma=0.99)\n",
    "scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optimizer_d, gamma=0.99)\n",
    "\n",
    "fixed_noise = torch.randn(64, LATENT_DIM, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef892b37",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Three epochs finish within the GPU runtime limit. Increase `NUM_EPOCHS` for stronger generators when working offline or scheduling longer runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c087889",
   "metadata": {},
   "source": [
    "### Training Strategy Notes\n",
    "- **Label smoothing:** Discriminator real labels use 0.9 and fake 0.1 to temper gradients and curb overconfidence.\n",
    "- **Preview snapshots:** The loop captures up to `MAX_PREVIEW_SNAPSHOTS` grids so we can inspect quality progression inline.\n",
    "- **Scheduler:** A mild exponential decay keeps learning stable across epochs; tweak `G_LR`, `D_LR`, or the schedulers to explore TTUR or longer runs.\n",
    "- **Extend depth:** Increase `GEN_FEATURES`/`DISC_FEATURES` for deeper models when GPU time budget allows.\n",
    "- **Further upgrades:** Consider feature matching, perceptual losses, or EMA on the generator weights as next experiments when iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "LOG_INTERVAL = 100\n",
    "CHECKPOINT_DIR = Path('/kaggle/working/checkpoints')\n",
    "SAMPLES_DIR = Path('/kaggle/working/samples')\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_g_loss = math.inf\n",
    "history = []\n",
    "preview_history = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    for step, (real_imgs, _) in enumerate(data_loader, start=1):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        batch_size = real_imgs.size(0)\n",
    "\n",
    "        noise = torch.randn(batch_size, LATENT_DIM, 1, 1, device=device)\n",
    "        fake_imgs = generator(noise).detach()\n",
    "        optimizer_d.zero_grad(set_to_none=True)\n",
    "        real_targets = torch.full((batch_size,), REAL_LABEL, device=device)\n",
    "        fake_targets = torch.full((batch_size,), FAKE_LABEL, device=device)\n",
    "        real_logits = discriminator(real_imgs)\n",
    "        fake_logits = discriminator(fake_imgs)\n",
    "        loss_d = criterion(real_logits, real_targets) + criterion(fake_logits, fake_targets)\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        noise = torch.randn(batch_size, LATENT_DIM, 1, 1, device=device)\n",
    "        optimizer_g.zero_grad(set_to_none=True)\n",
    "        generated = generator(noise)\n",
    "        gen_logits = discriminator(generated)\n",
    "        gen_targets = torch.full((batch_size,), GEN_TARGET, device=device)\n",
    "        loss_g = criterion(gen_logits, gen_targets)\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            print(f'Epoch {epoch:03d}/{NUM_EPOCHS} | Step {step:04d}/{len(data_loader)} | D: {loss_d.item():.3f} | G: {loss_g.item():.3f}')\n",
    "            with torch.no_grad():\n",
    "                preview = generator(fixed_noise).cpu()\n",
    "                preview_grid = utils.make_grid(preview, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "                utils.save_image(preview_grid, SAMPLES_DIR / f\"epoch_{epoch:03d}_step_{step:04d}.png\")\n",
    "                if len(preview_history) < MAX_PREVIEW_SNAPSHOTS:\n",
    "                    preview_history.append({\"epoch\": epoch, \"step\": step, \"grid\": preview_grid})\n",
    "\n",
    "    scheduler_g.step()\n",
    "    scheduler_d.step()\n",
    "    mean_g = loss_g.item()\n",
    "    history.append({\"epoch\": epoch, \"generator_loss\": mean_g, \"discriminator_loss\": loss_d.item()})\n",
    "    if mean_g < best_g_loss:\n",
    "        best_g_loss = mean_g\n",
    "        torch.save(generator.state_dict(), CHECKPOINT_DIR / 'generator_best.pt')\n",
    "        torch.save(discriminator.state_dict(), CHECKPOINT_DIR / 'discriminator_best.pt')\n",
    "        print(f'Saved new best generator checkpoint (epoch {epoch}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4309a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if history:\n",
    "    history_df = pd.DataFrame(history)\n",
    "    display(history_df.head())\n",
    "    ax = history_df.plot(x='epoch', y=['generator_loss', 'discriminator_loss'], marker='o', figsize=(6, 4))\n",
    "    ax.set_title('Training Losses by Epoch')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Run the training cell first to populate metrics.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf65c5",
   "metadata": {},
   "source": [
    "### Preview Evolution\n",
    "Review a handful of saved generator outputs to gauge visual improvements across training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if preview_history:\n",
    "    n_cols = 3\n",
    "    n_rows = math.ceil(len(preview_history) / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "    axes = np.atleast_1d(axes).flatten()\n",
    "    for ax, snapshot in zip(axes, preview_history):\n",
    "        ax.imshow(snapshot[\"grid\"].permute(1, 2, 0).numpy())\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Epoch {snapshot['epoch']} | Step {snapshot['step']}\")\n",
    "    for ax in axes[len(preview_history):]:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No preview grids captured yet. Run the training cell to populate preview_history.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc2034",
   "metadata": {},
   "source": [
    "## Generate Submission Images\n",
    "Sampling 8,000 latents matches the competition requirement. Images are streamed to `/kaggle/working/submission_images` and zipped into `images.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_IMAGES = 8_000\n",
    "GEN_BATCH = 64\n",
    "OUTPUT_ZIP = Path('/kaggle/working/images.zip')\n",
    "TEMP_IMAGE_DIR = Path('/kaggle/working/submission_images')\n",
    "\n",
    "checkpoint_path = CHECKPOINT_DIR / 'generator_best.pt'\n",
    "if checkpoint_path.exists():\n",
    "    generator.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "generator.eval()\n",
    "TEMP_IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    produced = 0\n",
    "    progress = tqdm(total=SUBMISSION_IMAGES, desc='Generating Monet-style images')\n",
    "    while produced < SUBMISSION_IMAGES:\n",
    "        current = min(GEN_BATCH, SUBMISSION_IMAGES - produced)\n",
    "        noise = torch.randn(current, LATENT_DIM, 1, 1, device=device)\n",
    "        fake_batch = generator(noise).cpu()\n",
    "        for idx in range(current):\n",
    "            filename = TEMP_IMAGE_DIR / f'monet_{produced + idx:05d}.jpg'\n",
    "            utils.save_image(fake_batch[idx], filename, normalize=True, value_range=(-1, 1))\n",
    "        produced += current\n",
    "        progress.update(current)\n",
    "    progress.close()\n",
    "\n",
    "with zipfile.ZipFile(OUTPUT_ZIP, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for img_path in sorted(TEMP_IMAGE_DIR.glob('*.jpg')):\n",
    "        zf.write(img_path, arcname=img_path.name)\n",
    "print(f'Created submission archive at {OUTPUT_ZIP}')\n",
    "\n",
    "for img_path in TEMP_IMAGE_DIR.glob('*.jpg'):\n",
    "    img_path.unlink()\n",
    "TEMP_IMAGE_DIR.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab7f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_EXPORT = Path('/kaggle/working/best_models.zip')\n",
    "FILES_TO_EXPORT = [\n",
    "    CHECKPOINT_DIR / 'generator_best.pt',\n",
    "    CHECKPOINT_DIR / 'discriminator_best.pt'\n",
    " ]\n",
    " \n",
    "with zipfile.ZipFile(MODEL_EXPORT, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for path in FILES_TO_EXPORT:\n",
    "        if path.exists():\n",
    "            zf.write(path, arcname=path.name)\n",
    "        else:\n",
    "            print(f'Skipping missing file: {path}')\n",
    " \n",
    "print(f'Packed checkpoints into {MODEL_EXPORT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f70b7e1",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "1. Use `Save & Run All` to generate a notebook version with `images.zip` and `best_models.zip` in the output tab.\n",
    "2. Submit the archive via `kaggle competitions submit -c gan-getting-started -f images.zip -m \"Your message\"` or upload it on the competition page.\n",
    "3. Download `best_models.zip` from the output tab to reuse the trained generator/discriminator in future experiments or publish them as a Kaggle Dataset.\n",
    "4. Track each submission in a change log (message + settings) so you can correlate leaderboard shifts with hyperparameter tweaks.\n",
    "5. Iterate on architecture or training schedule as needed for leaderboard improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
